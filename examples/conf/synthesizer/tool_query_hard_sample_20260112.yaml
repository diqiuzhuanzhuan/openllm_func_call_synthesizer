# settings controlling the synthesizer behaviour
name: "default_synthesizer"

mcp_servers:
  "ugreen_mcp":
    transport: "http://192.168.111.9:12000/mcp"

choose_part_tools: false # ["search_photos", "create_album"]

purpose: "function_call"  # or mcp_intent

query_generation:
  enable: False
  function_docs: examples/function_docs.json

  languages:
    - English
    - Chinese
    - Japanese
    - German
#    - French
#    - Spanish

  name: "function_query"
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", "parquet"

  providers:
    openai:
      backend: "openai"
      models:
        - "gpt-4o"
        - "gpt-4o-mini"
        - "gpt-4.1"
        - "gpt-5-mini"
      backend_params:
        max_tokens_per_minute: 1_000_000
        require_all_responses: False

    # anthropic:
    #   backend: "litellm"
    #   models:
    #     - "anthropic/claude-sonnet-4-20250514"
    #     - "anthropic/claude-sonnet-4-5-20250929"
    #   backend_params:
    #     max_tokens_per_minute: 1_000_000
    #     require_all_responses: False

    # ollama:
    #   backend: "litellm"
    #   models:
    #     - "ollama_chat/qwen3-1.7b"
    #   backend_params:
    #     max_tokens_per_minute: 1_000_000
    #     require_all_responses: False

    #

    google:
      backend: "openai"
      models:
        - "gemini-2.5-pro-nothinking"
        - "gemini-2.5-flash-thinking"
      backend_params:
        max_tokens_per_minute: 1_000_000
        require_all_responses: False

function_call_generation:
  enable: True
  function_dataset: "/data/work/LoongMa/data/ugreen/tool_query_hard_sample_20260112" #data set generated by query_generation
  max_num: -1 # max number of function calls to generate, -1 means all
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", "parquet"
  name: "tool_query_hard_sample_20260112"
  provider:
    model_name: "gpt-5-mini"
    backend: "openai"
    backend_params:
      require_all_responses: False
      max_tokens_per_minute: 8_000_000
    generation_params:
      max_tokens: 4096

critic:
  enable: True
  function_call_dataset: "data/tool_query_hard_sample_20260112"
  output_dir: "data"
  # cirtic need 'query', 'label', 'task_prompt'
  query_field: "query"
  task_prompt_field: "prompt"
  label_field: "function_call"
  functions_field: "functions"
  response_field: "answer"
  output_format: "jsonl"   # or "json", "csv", xlsx, "parquet"
  name: "tool_query_hard_sample_20260112_gpt_5_mini_critiqued_by_gpt_5_mini_2025_08_07"
  provider:
    model_name: "gpt-5-mini-2025-08-07"
    backend: "openai"
    backend_params:
      require_all_responses: False
      max_tokens_per_minute: 3_000_000


llama_factory:
  # required fields： answer， query， functions
  enable: True
  critic_dataset: "data/tool_query_hard_sample_20260112_gpt_5_mini_critiqued_by_gpt_5_mini_2025_08_07"
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", xlsx, "parquet"
  name: "tool_query_hard_sample_20260112_gpt_5_mini_critiqued_by_gpt_5_mini_2025_08_07_llama_factory"
  split_ratio: -1
  score_field: "score"
  score_threshold: 8
  system_prompt: "You are a helpful assistant."


verl:
  enable: False
  critic_dataset: "data/tool_query_hard_sample_20260112_gpt_5_mini_critiqued_by_gpt_5_mini_2025_08_07"
  output_dir: "data"
  output_format: "jsonl"
  name: "tool_query_hard_sample_20260112_gpt_5_mini_critiqued_by_gpt_5_mini_2025_08_07_verl"
  split_ratio: 0.85
  score_field: "score"
  score_threshold: 8
  system_prompt: "You are a helpful assistant."