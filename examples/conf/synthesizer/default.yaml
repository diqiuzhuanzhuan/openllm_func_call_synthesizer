# settings controlling the synthesizer behaviour
name: "default_synthesizer"

mcp_servers:
  "ugreen_mcp":
    transport: "http://192.168.111.9:12000/mcp"

choose_part_tools: false # ["search_photos", "create_album"]

query_generation:
  enable: True
  function_docs: examples/function_docs.json

  languages:
    - English
    - Chinese
    - Japanese
    - German
#    - French
#    - Spanish

  name: "function_query"
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", "parquet"

  providers:
    openai:
      backend: "openai"
      models:
        - "gpt-4o"
        - "gpt-4o-mini"
        - "gpt-5-mini-2025-08-07"
        - "gpt-4.1"
        - "gpt-5-mini"
      backend_params:
        max_tokens_per_minute: 1_000_000
        require_all_responses: False

    anthropic:
      backend: "litellm"
      models:
        - "anthropic/claude-sonnet-4-20250514"
        - "anthropic/claude-sonnet-4-5-20250929"
      backend_params:
        max_tokens_per_minute: 1_000_000
        require_all_responses: False

    google:
      backend: "openai"
      models:
        - "gemini-2.5-pro-nothinking"
        - "gemini-2.5-flash-thinking"
      backend_params:
        max_tokens_per_minute: 200_000
        require_all_responses: False

function_call_generation:
  enable: True
  function_dataset: "data/function_query" #data set generated by query_generation
  max_num: -1 # max number of function calls to generate, -1 means all
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", "parquet"
  name: "function_call_gpt_4o"
  provider:
    model_name: "gpt-4o"
    backend: "openai"
    backend_params:
      require_all_responses: False
      max_tokens_per_minute: 8_000_000

critic:
  enable: True
  function_call_dataset: "data/function_call_gpt_4o"
  output_dir: "data"
  # cirtic need 'query', 'label', 'task_prompt'
  query_field: "query"
  task_prompt_field: "prompt"
  label_field: "function_call"
  functions_field: "functions"
  response_field: "answer"
  output_format: "jsonl"   # or "json", "csv", xlsx, "parquet"
  name: "function_call_gpt_4o_critiqued_by_gpt_5_mini_2025_08_07"
  provider:
    model_name: "gpt-5-mini-2025-08-07"
    backend: "openai"
    backend_params:
      require_all_responses: False
      max_tokens_per_minute: 3_000_000


llama_factory:
  enable: True
  critic_dataset: "data/function_call_gpt_4o_critiqued_by_gpt_5_mini_2025_08_07"
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", xlsx, "parquet"
  name: "function_call_gpt_4o_critiqued_by_gpt_5_mini_2025_08_07_llama_factory"
  split_ratio: -1
  score_field: "score"
  score_threshold: 8
  system_prompt: "You are a helpful assistant. \
  You are given a query and a function call. You need to determine if the function call is correct for the query.\
  "
