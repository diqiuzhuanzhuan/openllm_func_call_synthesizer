# settings controlling the synthesizer behaviour
name: "default_synthesizer"

mcp_servers:
  "ugreen_mcp":
    transport: "http://192.168.111.9:12000/mcp"

query_generation:
  function_docs: examples/function_docs.json

  languages:
    - English
    - Chinese
    - Japanese
    - German
#    - French
#    - Spanish

  name: "function_query"
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", "parquet"

  providers:
    openai:
      backend: "openai"
      models:
        - "gpt-4o"
        - "gpt-4o-mini"
        - "gpt-5-mini-2025-08-07"
        - "gpt-4.1"
      backend_params:
        max_concurrent_requests: 100
        require_all_responses: False

    anthropic:
      backend: "openai"
      models:
        - "claude-sonnet-4-20250514"
        - "claude-sonnet-4-5-20250929"
      backend_params:
        max_concurrent_requests: 100
        require_all_responses: False

    google:
      backend: "openai"
      models:
        #- "gemini/gemini-2.5-pro-nothinking"
        - "gemini-2.5-pro-nothinking"
      backend_params:
        max_concurrent_requests: 100
        require_all_responses: False



function_call_generation:
  function_dataset: "function_call"
  max_num: -1 # max number of function calls to generate, -1 means all
  output_dir: "data"
  output_format: "jsonl"   # or "json", "csv", "parquet"
  #name: "data/ollama_ugreen_function_call"
  name: "gpt_4o"
  provider:
    model_name: "gpt-4o"
    backend: "openai"
    backend_params:
      require_all_responses: False
      max_concurrent_requests: 100


critic:
  function_call_dataset: "function_call/gpt_4o"
  output_dir: "data"
  # cirtic need 'query', 'label', 'task_prompt'
  query_field: "query"
  task_prompt_field: "prompt"
  label_field: "function_call"
  functions_field: "functions"
  response_field: "answer"
  output_format: "jsonl"   # or "json", "csv", xlsx, "parquet"
  name: "critic_gpt_4o_gpt_5_mini_2025_08_07"
  provider:
    model_name: "gpt-5-mini-2025-08-07"
    backend: "openai"
    backend_params:
      require_all_responses: False
      max_concurrent_requests: 100
