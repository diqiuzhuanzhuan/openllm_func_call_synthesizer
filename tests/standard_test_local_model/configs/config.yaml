# ================================================================================
# LLM Function Call Synthesizer Configuration File
# 用于 evaluate_local_model.py 的配置文件
# ================================================================================
#
# 使用说明：
# 1. 本配置文件支持两种运行模式：本地模型模式 和 API模式
# 2. 通过设置 use_api 字段来切换模式
# 3. 本地模式：直接加载本地模型到GPU进行推理（需要GPU资源）
# 4. API模式：通过HTTP API调用远程模型服务（无需GPU）
#
# 运行命令：
#   python evaluate_local_model.py  config.yaml
#
# ================================================================================

# ========== 运行模式配置 ==========
# 选择使用本地模型还是API模式
# false 或不设置 = 本地模型模式（需要GPU）
# true = API模式（通过HTTP API调用远程模型）
use_api: true
model_type:  ollama_api_no_tool # ollama_api_function_call # vllm_api # "local" or "ollama_api" or "vllm_api"

tool_path: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/openai_tools.json"
system_prompt: "You are a helpful assistant. "
# ========== 本地模型配置 ==========
# 仅在 use_api: false 时需要配置
cuda_device: "0,1,4,5,7"
num_processes: 4

# 本地模型路径
# model_file_path: "/data0/work/SusieSu/project/workspace/llama/LLaMA-Factory/saves/qwen3_1.7b_1030_intent_mcp/sft/checkpoint-240"
model_file_path: "/data0/work/SusieSu/project/workspace/LLaMA-Factory-main/saves/qwen3_1.7b_1205_function_call_batch8/sft/checkpoint-600"

# ========== API模式配置 ==========
# 仅在 use_api: true 时需要配置  API服务地址 (支持Ollama格式的API)
# api_url: "http://192.168.111.3:8019/api/chat"
# api_url: "http://localhost:11434"
api_url: "http://localhost:11434/v1/chat/completions"
# API模型名称 (与本地模型路径不同，这里是API服务中的模型名)
model_name: "function_call_1216_no_tool"  #"function_call_1216-q4_K_M"
# model_name: "mcp_intent_1016:f16"

# API调用参数（可选，默认值如下）
temperature: 0.01

# 通过指定 root 目录来简化路径
root: "/data0/work/SusieSu/project/openllm_datas_and_temp_codes/data_1226"
output_root: "/data0/work/SusieSu/project/openllm_datas_and_temp_codes/data_1226"

# 选择使用哪个系统提示词（仅用于向后兼容）
prompt_key: "mcp"  # uliya or mcp

# ========== 新增：混合/分割模式配置 ==========
# mode: 选择运行模式
#   - "hybrid": 混合模式，先用 mcp_input 请求，如果没有结果再用 uliya_input
#   - "split": 分割模式，根据 gt_intent 是否在 mcp_intent_list/uliya_intent_list 中选择对应 prompt
mode: "hybrid"  # hybrid or split

# split 模式需要的配置：定义哪些 intent 使用哪个 prompt
mcp_intent_list:
-  "unknown"
-  "video_search_control"
-  "video_play_control"
-  "create_album"
-  "search_photos"
-  "music_play_control"
-  "get_system_info"
-  "music_settings_control"
-  "get_album_list"
-  "summary_document"
-  "translate"
-  "music_search_control"
-  "adjust_led_brightness"
-  "start_file_backup_sync_ui"
-  "set_power_efficiency_mode"
-  "configure_nas_power_settings"
-  "docker_search_container"
-  "shutdown_nas_device"
-  "configure_memory_compression"
-  "configure_device_buzzer_alerts"
-  "control_cooling_fan_speed"
-  "docker_search_image"
-  ""
# ========== 输入输出文件配置 ==========
# 支持 ${root} 变量，代码要先解析
input_file: "${root}/mcp_test_fc.xlsx"
output_file: "${output_root}/test_all_llm_response.xlsx"

# 数据后处理输入输出
postprocess_input_file: "${output_root}/test_all_llm_response.xlsx"
postprocess_output_file: "${output_root}/test_all_processed_hybrid.xlsx"

# 评测输入输出文件
evaluate_input_file: "${output_root}/test_all_processed_hybrid.xlsx"
evaluate_output_file: "${output_root}/test_all_evaluate_hybrid.xlsx"

confusion_matrix_file: "${output_root}/test_all_evaluate_hybrid_confusion_matrix.xlsx"

# #evaluate_output_str 输入输出文件
# evaluate_output_str_input_file: "${root}/test_mcp_1017_with_gt_v3.xlsx"
# evaluate_output_str_output_file: "${root}/test_mcp_1017_final_v3.xlsx"

# ========== 步骤执行控制 ==========
# 控制执行哪些步骤 (true/false)
steps:
  inference: false               # 是否执行LLM推理
  postprocess: false             # 是否执行数据后处理（LLM+GT标准字段合成一步）
  evaluate: true                  # 是否执行评测
  evaluate_output_str: false

# ========== 评测模式配置 ==========
# evaluate_separate: 是否分开评测 MCP 和 Uliya 数据
#   - false (默认): 合并评测所有数据
#   - true: 根据 gt_intent 是否在 mcp_intent_list/uliya_intent_list 中分开评测
evaluate_separate: false  # 设为 true 则分开评测

# ========== 数据字段配置 ==========
# 输入数据中包含用户查询的字段名
input_field: "query"

# Ground Truth相关字段名
ground_truth: "function"    # 数据中 ground truth 列名
ground_truth_intent: "gt_intent"    # 数据中 ground truth intent 列名， 这个如果没有 会从ground_truth来提取intent
ground_truth_slot: "gt_slots"    # 数据中 ground truth slot 列名， 这个如果没有 会从ground_truth来提取slots

# LLM输出相关字段名
llm_intent: "llm_intent"    # 数据中 llm intent 列名， 这个如果没有 会从 llm_response 来提取intent, inference 结果 会保存到 llm_response列。
llm_slot: "llm_slots"    # 数据中 llm slot 列名， 这个如果没有 会从llm_response来提取slots

# ========== 系统提示词配置 ==========
# # 方式1: 从JSON文件读取
# json_prompt_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/examples/prompt_dict/prompt_dict.json"  # JSON文件路径

## 方式2: 从txt 读取prompt
system_prompt_mcp_file:  "/data0/work/SusieSu/project/openllm_func_call_synthesizer/examples/prompt_dict/function_call_prompt.txt"
system_prompt_uliya_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/examples/prompt_dict/function_call_prompt.txt"

# # ========== 评测配置 ==========
# # 是否进行评测 (已被steps.evaluate替代，保留兼容性)
# do_evaluate: true

# ========== 高级配置 ==========
# 生成参数配置（可选，代码中已有默认值）
generation_config:
  max_new_tokens: 5000
  temperature: 0.01
  # top_p: 0.1

# 日志配置
logging:
  level: "INFO"
  save_logs: true
  log_file: "/data1/work/LiuWeiZhong/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/test_result"

evaluate_output_str:
  # 针对纯字符串输出的评测：直接比较两列是否相等
  ground_truth_col: "intent"      # Ground Truth 所在列名
  llm_intent_col: "llm_response"  # LLM 纯字符串输出所在列名

# ================================================================================
# 配置示例
# ================================================================================
#
# 【示例1：使用本地模型】
# use_api: false
# cuda_device: "0"
# model_name: "/path/to/your/local/model"
#
# 【示例2：使用API模式】
# use_api: true
# api_url: "http://192.168.111.3:11434/api/chat"
# model_name: "mcp_intent_1016:q4_k_m"  # API中的模型名
# top_p: 0.1
# temperature: 0.01
#
# 【示例3：只执行推理】
# steps:
#   inference: true
#   postprocess: false
#   evaluate: false
#
# 【示例4：完整流程（推理+后处理+评测）】
# steps:
#   inference: true
#   postprocess: true
#   evaluate: true
#
# ================================================================================
# 更多文档：
#   - README.md: 项目主文档
#   - QUICKSTART.md: 快速开始指南
#   - 使用说明_API模式.md: API模式详细说明
# ================================================================================
