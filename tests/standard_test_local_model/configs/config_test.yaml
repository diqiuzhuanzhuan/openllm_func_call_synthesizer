# ================================================================================
# LLM Function Call Synthesizer Configuration File
# 用于 evaluate_local_model.py 的配置文件
# ================================================================================
#
# 使用说明：
# 1. 本配置文件支持两种运行模式：本地模型模式 和 API模式
# 2. 通过设置 use_api 字段来切换模式
# 3. 本地模式：直接加载本地模型到GPU进行推理（需要GPU资源）
# 4. API模式：通过HTTP API调用远程模型服务（无需GPU）
#
# 运行命令：
#   python evaluate_local_model.py --config config.yaml
#
# ================================================================================

# ========== 运行模式配置 ==========
# 选择使用本地模型还是API模式
# false 或不设置 = 本地模型模式（需要GPU）
# true = API模式（通过HTTP API调用远程模型）
use_api: false

# ========== 本地模型配置 ==========
# 仅在 use_api: false 时需要配置
cuda_device: "1,2,5,7"

# 本地模型路径
# model_file_path: "/data0/work/SusieSu/project/workspace/llama/LLaMA-Factory/saves/qwen3_1.7b_1025_intent_mcp/stf/checkpoint-1900"
# model_file_path: "/data0/work/SusieSu/project/workspace/llama/LLaMA-Factory/saves/qwen3_1.7b_1025_intent_mcp/stf/checkpoint-500"
model_file_path: "/data0/work/SusieSu/project/workspace/llama/LLaMA-Factory/saves/qwen3_1.7b_1027_intent_mcp_v1/stf/checkpoint-100"

# ========== API模式配置 ==========
# 仅在 use_api: true 时需要配置  API服务地址 (支持Ollama格式的API)
api_url: "http://192.168.111.3:11434/api/chat"

# API模型名称 (与本地模型路径不同，这里是API服务中的模型名)
model_name: "mcp_intent_1016:q4_k_m"
# model_name: "mcp_intent_1016:f16"

# API调用参数（可选，默认值如下）
temperature: 0.01

root: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/mcp_dataprocess_1025/train_datas_1025"

# ========== 输入输出文件配置 ==========
# llm推理文件 输入输出
# input_file: "${root}/test_mcp_intent.xlsx"
input_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/mcp_dataprocess_1025/train_datas_1025/test_mcp_intent.xlsx"
output_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/mcp_dataprocess_1025/train_datas_1025/test_mcp_intent_llm_v3.xlsx"

# 数据后处理输入输出。 输入为LLM推理输出，输出为增加GT/LLM各标准字段后的文件（作为评测输入）
postprocess_input_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/mcp_dataprocess_1025/train_datas_1025/test_mcp_intent_llm_v3.xlsx"
postprocess_output_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/mcp_dataprocess_1025/train_datas_1025/test_mcp_intent_llm_processed_v3.xlsx"

# 评测输入输出文件
# evaluate_input_file: "${root}/test_mcp_intent_llm_processed_v3.xlsx"
evaluate_input_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/mcp_dataprocess_1025/train_datas_1025/test_mcp_intent_llm_processed_v3.xlsx"
evaluate_output_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/mcp_dataprocess_1025/train_datas_1025/test_mcp_intent_llm_evaluate_v3.xlsx"

# #evaluate_output_str 输入输出文件
# evaluate_output_str_input_file: "${root}/test_mcp_1017_with_gt_v3.xlsx"
# evaluate_output_str_output_file: "${root}/test_mcp_1017_final_v3.xlsx"

# ========== 步骤执行控制 ==========
# 控制执行哪些步骤 (true/false)
steps:
  inference: true                 # 是否执行LLM推理
  postprocess: true             # 是否执行数据后处理（LLM+GT标准字段合成一步）
  evaluate: true                  # 是否执行评测
  evaluate_output_str: false
# ========== 数据字段配置 ==========
# 输入数据中包含用户查询的字段名
input_field: "input"

# Ground Truth相关字段名
ground_truth: "output"    # 数据中 ground truth 列名
ground_truth_intent: "gt_intent"    # 数据中 ground truth intent 列名， 这个如果没有 会从ground_truth来提取intent
ground_truth_slot: "gt_slots"    # 数据中 ground truth slot 列名， 这个如果没有 会从ground_truth来提取slots

# LLM输出相关字段名
llm_intent: "llm_intent"    # 数据中 llm intent 列名， 这个如果没有 会从llm_response来提取intent, inference 结果 会保存到 llm_response列。
llm_slot: "llm_slots"    # 数据中 llm slot 列名， 这个如果没有 会从llm_response来提取slots

# ========== 系统提示词配置 ==========
# 方式1: 直接在这里写prompt（已注释）
# 方式2: 从JSON文件读取（推荐）
# 设置 prompt_file 和 prompt_key 来从JSON文件读取
prompt_file: "prompt_dict.json"  # JSON文件路径
prompt_key: "mcp_system_prompt"  # JSON中的key名称

# 如果不使用JSON文件，可以直接在这里写system_prompt


# # ========== 评测配置 ==========
# # 是否进行评测 (已被steps.evaluate替代，保留兼容性)
# do_evaluate: true

# ========== 高级配置 ==========
# 生成参数配置（可选，代码中已有默认值）
generation_config:
  max_new_tokens: 5000
  temperature: 0.01
  # top_p: 0.1

# 日志配置
logging:
  level: "INFO"
  save_logs: true
  log_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/test_result"

evaluate_output_str:
  # 针对纯字符串输出的评测：直接比较两列是否相等
  ground_truth_col: "intent"      # Ground Truth 所在列名
  llm_intent_col: "llm_response"  # LLM 纯字符串输出所在列名

# ================================================================================
# 配置示例
# ================================================================================
#
# 【示例1：使用本地模型】
# use_api: false
# cuda_device: "0"
# model_name: "/path/to/your/local/model"
#
# 【示例2：使用API模式】
# use_api: true
# api_url: "http://192.168.111.3:11434/api/chat"
# model_name: "mcp_intent_1016:q4_k_m"  # API中的模型名
# top_p: 0.1
# temperature: 0.01
#
# 【示例3：只执行推理】
# steps:
#   inference: true
#   postprocess: false
#   evaluate: false
#
# 【示例4：完整流程（推理+后处理+评测）】
# steps:
#   inference: true
#   postprocess: true
#   evaluate: true
#
# ================================================================================
# 更多文档：
#   - README.md: 项目主文档
#   - QUICKSTART.md: 快速开始指南
#   - 使用说明_API模式.md: API模式详细说明
# ================================================================================
