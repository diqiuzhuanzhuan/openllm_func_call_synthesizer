# ================================================================================
# LLM Function Call Synthesizer Configuration File
# 用于 evaluate_local_model.py 的配置文件
# ================================================================================
#
# 使用说明：
# 1. 本配置文件支持两种运行模式：本地模型模式 和 API模式
# 2. 通过设置 use_api 字段来切换模式
# 3. 本地模式：直接加载本地模型到GPU进行推理（需要GPU资源）
# 4. API模式：通过HTTP API调用远程模型服务（无需GPU）
#
# 运行命令：
#   python evaluate_local_model.py --config config.yaml
#
# ================================================================================

# ========== 运行模式配置 ==========
# 选择使用本地模型还是API模式
# false 或不设置 = 本地模型模式（需要GPU）
# true = API模式（通过HTTP API调用远程模型）
use_api: false

# ========== 本地模型配置 ==========
# 仅在 use_api: false 时需要配置
cuda_device: "0,1,2,3,4,5"
num_processes: 6

# 本地模型路径
# model_file_path: "/data0/work/SusieSu/project/workspace/llama/LLaMA-Factory/saves/qwen3_1.7b_1030_intent_mcp/sft/checkpoint-240"
model_file_path: "/data0/work/SusieSu/project/workspace/weizhong_LLaMA-Factory-main/saves/qwen3_1.7b_1119_intent_mcp_1057/sft/checkpoint-600"

# ========== API模式配置 ==========
# 仅在 use_api: true 时需要配置  API服务地址 (支持Ollama格式的API)
api_url: "http://192.168.111.3:11434/api/chat"

# API模型名称 (与本地模型路径不同，这里是API服务中的模型名)
model_name: "mcp_intent_1016:q4_k_m"
# model_name: "mcp_intent_1016:f16"

# API调用参数（可选，默认值如下）
temperature: 0.01

# 通过指定 root 目录来简化路径
root: "/data0/work/SusieSu/project/workspace/weizhong_LLaMA-Factory-main/data/mcp_train_data/mcp_data_1119_for_train_v1"
output_root: "/data1/work/LiuWeiZhong/mcp_data_1118_for_train/new_evaluation_1119_1335"

# 选择使用哪个系统提示词（仅用于向后兼容）
prompt_key: "mcp"  # uliya or mcp

# ========== 新增：混合/分割模式配置 ==========
# mode: 选择运行模式
#   - "hybrid": 混合模式，先用 mcp_input 请求，如果没有结果再用 uliya_input
#   - "split": 分割模式，根据 gt_intent 是否在 mcp_intent_list/uliya_intent_list 中选择对应 prompt
mode: "hybrid"  # hybrid or split

# split 模式需要的配置：定义哪些 intent 使用哪个 prompt
mcp_intent_list:
  - "create_album"
  - "search_photos"
  - "get_album_list"
  - "music_play_control"
  - "music_search_control"
  - "music_settings_control"
  - "video_search_control"
  - "video_play_control"
  - "get_system_info"
  - "general_query"
  - "summary_document"
  - "translate"

# uliya_intent_list:
#   - "general_query"
#   - "summary_document"
#   # - "search_document"
#   - "translate"

# ========== 输入输出文件配置 ==========
# 支持 ${root} 变量，代码要先解析
input_file: "${root}/test_all.xlsx"
output_file: "${output_root}/test_all_output_hybrid.xlsx"

# 数据后处理输入输出
postprocess_input_file: "${output_root}/test_all_output_hybrid.xlsx"
postprocess_output_file: "${output_root}/test_all_processed_hybrid.xlsx"

# 评测输入输出文件
evaluate_input_file: "${output_root}/test_all_processed_hybrid.xlsx"
evaluate_output_file: "${output_root}/test_all_evaluate_hybrid.xlsx"

# #evaluate_output_str 输入输出文件
# evaluate_output_str_input_file: "${root}/test_mcp_1017_with_gt_v3.xlsx"
# evaluate_output_str_output_file: "${root}/test_mcp_1017_final_v3.xlsx"

# ========== 步骤执行控制 ==========
# 控制执行哪些步骤 (true/false)
steps:
  inference: true               # 是否执行LLM推理
  postprocess: true             # 是否执行数据后处理（LLM+GT标准字段合成一步）
  evaluate: true                  # 是否执行评测
  evaluate_output_str: false

# ========== 评测模式配置 ==========
# evaluate_separate: 是否分开评测 MCP 和 Uliya 数据
#   - false (默认): 合并评测所有数据
#   - true: 根据 gt_intent 是否在 mcp_intent_list/uliya_intent_list 中分开评测
evaluate_separate: false  # 设为 true 则分开评测

# ========== 数据字段配置 ==========
# 输入数据中包含用户查询的字段名
input_field: "input"

# Ground Truth相关字段名
ground_truth: "output"    # 数据中 ground truth 列名
ground_truth_intent: "gt_intent"    # 数据中 ground truth intent 列名， 这个如果没有 会从ground_truth来提取intent
ground_truth_slot: "gt_slots"    # 数据中 ground truth slot 列名， 这个如果没有 会从ground_truth来提取slots

# LLM输出相关字段名
llm_intent: "llm_intent"    # 数据中 llm intent 列名， 这个如果没有 会从 llm_response 来提取intent, inference 结果 会保存到 llm_response列。
llm_slot: "llm_slots"    # 数据中 llm slot 列名， 这个如果没有 会从llm_response来提取slots

# ========== 系统提示词配置 ==========
# # 方式1: 从JSON文件读取
# json_prompt_file: "/data0/work/SusieSu/project/openllm_func_call_synthesizer/examples/prompt_dict/prompt_dict.json"  # JSON文件路径

## 方式2: 从txt 读取prompt
system_prompt_mcp_file: "/data1/work/LiuWeiZhong/openllm_func_call_synthesizer/examples/prompt_dict/system_prompt_mcp_1118_2031.txt"
system_prompt_uliya_file: "/data1/work/LiuWeiZhong/openllm_func_call_synthesizer/examples/prompt_dict/system_prompt_mcp_1118_2031.txt"


# 如果不使用JSON文件，可以直接在这里写system_prompt


# # ========== 评测配置 ==========
# # 是否进行评测 (已被steps.evaluate替代，保留兼容性)
# do_evaluate: true

# ========== 高级配置 ==========
# 生成参数配置（可选，代码中已有默认值）
generation_config:
  max_new_tokens: 5000
  temperature: 0.01
  # top_p: 0.1

# 日志配置
logging:
  level: "INFO"
  save_logs: true
  log_file: "/data1/work/LiuWeiZhong/openllm_func_call_synthesizer/src/openllm_func_call_synthesizer/data_process/test_result"

evaluate_output_str:
  # 针对纯字符串输出的评测：直接比较两列是否相等
  ground_truth_col: "intent"      # Ground Truth 所在列名
  llm_intent_col: "llm_response"  # LLM 纯字符串输出所在列名

# ================================================================================
# 配置示例
# ================================================================================
#
# 【示例1：使用本地模型】
# use_api: false
# cuda_device: "0"
# model_name: "/path/to/your/local/model"
#
# 【示例2：使用API模式】
# use_api: true
# api_url: "http://192.168.111.3:11434/api/chat"
# model_name: "mcp_intent_1016:q4_k_m"  # API中的模型名
# top_p: 0.1
# temperature: 0.01
#
# 【示例3：只执行推理】
# steps:
#   inference: true
#   postprocess: false
#   evaluate: false
#
# 【示例4：完整流程（推理+后处理+评测）】
# steps:
#   inference: true
#   postprocess: true
#   evaluate: true
#
# ================================================================================
# 更多文档：
#   - README.md: 项目主文档
#   - QUICKSTART.md: 快速开始指南
#   - 使用说明_API模式.md: API模式详细说明
# ================================================================================
